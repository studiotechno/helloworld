# Story 4.1: Vue d'Ensemble des Features du Projet

## Status

Draft

## Story

**As a** PM,
**I want** demander quelles sont les features principales de mon projet,
**So that** je comprenne rapidement ce que fait l'application.

## Acceptance Criteria

1. **Given** j'ai un repository connecte, **When** je demande "C'est quoi les features principales ?" ou equivalent, **Then** le systeme analyse le code et repond avec une liste structuree des features
2. **And** chaque feature est expliquee en termes metier (pas techniques)
3. **And** les fichiers/dossiers cles sont cites pour chaque feature
4. **And** la reponse arrive en moins de 30 secondes
5. **Given** le repository est vide ou minimal, **When** je demande les features, **Then** le systeme indique qu'il n'y a pas assez de code pour identifier des features
6. **And** il suggere ce qu'il faudrait pour avoir une analyse pertinente

## Tasks / Subtasks

- [ ] Task 1: Create Feature Analysis Prompt Template (AC: 1, 2, 3)
  - [ ] Create `lib/ai/prompts/features.ts` module
  - [ ] Define `FEATURE_ANALYSIS_PROMPT` with instructions for business-oriented feature extraction
  - [ ] Include guidelines for structuring feature lists
  - [ ] Add instructions for citing relevant files/folders per feature
  - [ ] Add examples of good vs bad feature descriptions

- [ ] Task 2: Implement Feature Detection Logic (AC: 1, 2, 3)
  - [ ] Create `lib/ai/analysis/features.ts` module
  - [ ] Implement `buildFeatureAnalysisContext()` function
  - [ ] Add logic to extract project structure context (folders, key files)
  - [ ] Include package.json/dependencies context if available
  - [ ] Add README content extraction if available

- [ ] Task 3: Integrate Feature Analysis with Chat API (AC: 1, 4)
  - [ ] Update `lib/ai/prompts/base.ts` to support analysis-specific prompts
  - [ ] Create prompt selector based on question type detection
  - [ ] Ensure streaming works correctly with analysis prompts
  - [ ] Verify response time < 30 seconds

- [ ] Task 4: Handle Edge Cases (AC: 5, 6)
  - [ ] Add empty/minimal repository detection logic
  - [ ] Create appropriate response for minimal codebases
  - [ ] Add suggestions for what would enable better analysis
  - [ ] Test with empty and minimal repositories

- [ ] Task 5: Unit Tests (AC: 1-6)
  - [ ] Test feature analysis prompt generation
  - [ ] Test context building functions
  - [ ] Test empty repository handling
  - [ ] Manual testing with real codebase questions

## Dev Notes

### Previous Story Context (3.8)

Story 3.8 implemented:
- AnalysisLoader component with phased feedback
- Visual feedback for long-running analyses (>2s)
- Progress indicators showing files/folders analyzed
- Timeout handling after 30 seconds

Key files from Story 3.8:
- `components/chat/AnalysisLoader.tsx`
- `hooks/use-analysis-loader.ts`

### Existing Implementation Analysis

**Current Prompt System** - `lib/ai/prompts/base.ts`:
- `buildBaseSystemPrompt()` - Core PM-friendly prompt
- `buildSystemPromptWithContext()` - Adds repository context
- Already includes citation format instructions

**Current Chat API** - `app/api/chat/route.ts`:
- Uses `buildSystemPromptWithContext()` for system prompt
- Streams responses via Vercel AI SDK
- Supports repository context (name, branch)

**Current Confidence System** - `lib/ai/confidence.ts`:
- Binary confidence handling (>= 80% certain vs uncertain)
- Pedagogical response instructions
- Professional vocabulary guidelines

### Feature Analysis Implementation Strategy

The feature analysis is primarily a **prompt engineering task** since we don't have direct code access from the frontend. The LLM will analyze based on:
1. Repository metadata (name, description)
2. Conversation history and user questions
3. Any context the user provides

For MVP, the feature analysis relies on the LLM's ability to:
- Infer features from repository structure descriptions
- Ask clarifying questions if context is insufficient
- Structure responses in business terms

### Proposed Prompt Structure

```typescript
// lib/ai/prompts/features.ts
export const FEATURE_ANALYSIS_PROMPT = `
## Analyse des Features

Quand l'utilisateur demande les features principales du projet:

1. **Liste structuree** - Presente chaque feature avec:
   - Nom de la feature (en termes metier)
   - Description courte (1-2 phrases, comprehensible par un non-dev)
   - Fichiers/dossiers cles associes [citations]

2. **Priorisation** - Commence par les features les plus importantes/visibles

3. **Vocabulaire** - Parle "produit", pas "technique":
   - BON: "Systeme de paiement permettant aux clients de regler par carte"
   - MAUVAIS: "Integration Stripe avec webhooks et charge creation"

4. **Citations** - Pour chaque feature, cite les fichiers cles:
   - "[src/payments/]" pour un module
   - "[app/api/checkout/route.ts]" pour un fichier specifique

### Si le repository semble vide ou minimal:

Reponds avec:
"Je ne detecte pas suffisamment de code pour identifier des features distinctes.

Pour une analyse pertinente, le projet devrait contenir:
- Des routes ou endpoints API
- Des composants UI (si frontend)
- Des modeles de donnees ou schemas

Peux-tu me donner plus de contexte sur ce que fait ce projet ?"
`
```

### Project Structure for This Story

```
lib/
├── ai/
│   ├── prompts/
│   │   ├── base.ts            # UPDATE: Add feature prompt integration
│   │   ├── features.ts        # NEW: Feature analysis prompt
│   │   └── index.ts           # UPDATE: Export features
│   └── analysis/
│       └── features.ts        # NEW: Feature analysis context builder
```

### Key Constraints

1. **No Direct Code Access** - LLM analyzes based on context, not raw code
2. **30 Second Limit** - Must respond within NFR2 constraint
3. **French Language** - All responses in French
4. **Streaming Compatible** - Must work with SSE
5. **Binary Confidence** - Apply existing confidence rules

### Integration with Epic 3 Components

This story builds on Epic 3's chat infrastructure:
- Uses same `ChatMessage` component
- Uses same streaming mechanism
- Uses `AnalysisLoader` for feedback (if >2s)
- Uses same citation format `[path/to/file.ext]`

## Testing

### Test File Locations

- `lib/ai/prompts/features.test.ts` - Feature prompt tests
- `lib/ai/analysis/features.test.ts` - Context builder tests

### Test Standards

- Test framework: Vitest + Testing Library
- Co-located tests next to source files
- Test prompt contains required elements (structure, vocabulary, citations)
- Manual testing for actual LLM behavior

### Manual Testing Checklist

1. [ ] Ask "C'est quoi les features principales ?" - get structured list
2. [ ] Ask "Que fait ce projet ?" - get business-oriented response
3. [ ] Ask "Quelles sont les fonctionnalites ?" - get similar response
4. [ ] Verify each feature includes file citations
5. [ ] Verify vocabulary is PM-friendly (not too technical)
6. [ ] Verify response arrives in < 30 seconds
7. [ ] Test with minimal context - get appropriate uncertainty response
8. [ ] Verify AnalysisLoader appears for longer responses

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-11 | 0.1 | Initial story draft | SM Agent |

## Dev Agent Record

### Agent Model Used
_To be filled during implementation_

### Debug Log References
_To be filled during implementation_

### Completion Notes List
_To be filled during implementation_

### File List
_To be filled during implementation_

## QA Results
_To be filled by QA Agent_
